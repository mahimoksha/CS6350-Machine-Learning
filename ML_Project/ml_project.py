# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJTCtq1nYNmVE_GXekzsBuqwhwc0Qjxx
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import  StandardScaler,MinMaxScaler,OneHotEncoder
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from scipy.sparse import hstack
import xgboost as xgb
from sklearn.svm import SVC
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
import warnings
warnings.filterwarnings("ignore")

# train_data = pd.read_csv("train_final.csv")
# test_data = pd.read_csv("test_final.csv")
train_data = pd.read_csv("train_final.csv")
test_data = pd.read_csv("test_final.csv")

# data_mean, data_std = np.mean(train_data['age']), np.std(train_data['age'])
# cut_off = data_std * 5
# lower, upper = data_mean - cut_off, data_mean + cut_off
# outliers_age = []
# for x in range(len(train_data['age'])):
#    if train_data['age'].iloc[x] < lower or train_data['age'].iloc[x] > upper:
#      outliers_age.append(x)
# print('Identified outliers age: %d' % len(outliers_age))


# data_mean, data_std = np.mean(train_data['education.num']), np.std(train_data['education.num'])
# cut_off = data_std * 5
# lower, upper = data_mean - cut_off, data_mean + cut_off
# outliers_education = []
# for x in range(len(train_data['education.num'])):
#    if train_data['education.num'].iloc[x] < lower or train_data['education.num'].iloc[x] > upper:
#      outliers_education.append(x)
# print('Identified outliers age: %d' % len(outliers_education))


# data_mean, data_std = np.mean(train_data['fnlwgt']), np.std(train_data['fnlwgt'])
# cut_off = data_std * 5
# lower, upper = data_mean - cut_off, data_mean + cut_off
# outliers_fnlwgt = []
# for x in range(len(train_data['fnlwgt'])):
#    if train_data['fnlwgt'].iloc[x] < lower or train_data['fnlwgt'].iloc[x] > upper:
#      outliers_fnlwgt.append(x)
# print('Identified outliers fnlwgt: %d' % len(outliers_fnlwgt))

# data_mean, data_std = np.mean(train_data['capital.gain']), np.std(train_data['capital.gain'])
# cut_off = data_std * 5
# lower, upper = data_mean - cut_off, data_mean + cut_off
# outliers_capital_gain = []
# for x in range(len(train_data['capital.gain'])):
#    if train_data['capital.gain'].iloc[x] < lower or train_data['capital.gain'].iloc[x] > upper:
#      outliers_capital_gain.append(x)
# print('Identified outliers capital gain: %d' % len(outliers_capital_gain))

# data_mean, data_std = np.mean(train_data['capital.loss']), np.std(train_data['capital.loss'])
# cut_off = data_std * 5
# lower, upper = data_mean - cut_off, data_mean + cut_off
# outliers_capital_loss = []
# for x in range(len(train_data['capital.loss'])):
#    if train_data['capital.loss'].iloc[x] < lower or train_data['capital.loss'].iloc[x] > upper:
#      outliers_capital_loss.append(x)
# print('Identified outliers capital loss: %d' % len(outliers_capital_loss))

# data_mean, data_std = np.mean(train_data['hours.per.week']), np.std(train_data['hours.per.week'])
# cut_off = data_std * 5
# lower, upper = data_mean - cut_off, data_mean + cut_off
# outliers_hpm = []
# for x in range(len(train_data['hours.per.week'])):
#    if train_data['hours.per.week'].iloc[x] < lower or train_data['hours.per.week'].iloc[x] > upper:
#      outliers_hpm.append(x)
# print('Identified outliers hpm: %d' % len(outliers_hpm))
# new_train_data = []
# for i in range(len(train_data)):
#     if i not in outliers_hpm:
#        if i not in outliers_capital_loss:  
#           if i not in outliers_capital_gain:
#              if i not in outliers_fnlwgt: 
#                 if i not in outliers_age:
#                    if i not in outliers_education:
#                       new_train_data.append(train_data.iloc[i].values)
# ntrain_data = pd.DataFrame(new_train_data)
# ntrain_data.columns = train_data.columns
# train_data = ntrain_data

weight_1 = 6016/18984
weight_2 = 6016/(6016+18984)

missing_features = []
for i in train_data.columns:
    count = sum(train_data[i]=='?')
    if count!=0:
        missing_features.append(i)

print("missing features",missing_features)

def majority_feature_yes(feature):
    yes = dict(train_data[train_data['income>50K']==1][feature].value_counts())
    majority_fea_yes = sorted(yes,key=yes.get,reverse=True)[0]
    return majority_fea_yes
def majority_feature_no(feature):
    no = dict(train_data[train_data['income>50K']==0][feature].value_counts())
    majority_fea_no = sorted(no,key=no.get,reverse=True)[0]
    return majority_fea_no

#Handling missing values workclass,occupation,native.country
print("missing values and corresponding majority count")
for fea in missing_features:
    yes_attr_data = train_data[train_data['income>50K']==1]
    no_attr_data = train_data[train_data['income>50K']==0]
    majority_attr_yes = majority_feature_yes(fea)
    majority_attr_no = majority_feature_no(fea)
    print(fea,majority_attr_yes,majority_attr_no)
    yes_attr_data[fea].replace('?',majority_attr_yes,inplace=True)
    no_attr_data[fea].replace('?',majority_attr_no,inplace=True)
    train_data = pd.concat([yes_attr_data,no_attr_data])
    train_data = train_data.sample(frac=1).reset_index(drop=True)

train_data

sns.set_style("whitegrid");
sns.FacetGrid(train_data, hue="income>50K", size=4) \
   .map(plt.scatter, "age", "education.num") \
   .add_legend();
plt.show();

sns.set_style("whitegrid");
sns.FacetGrid(train_data, hue="income>50K", size=4) \
   .map(plt.scatter, "hours.per.week", "education.num") \
   .add_legend();
plt.show();

sns.set_style("whitegrid");
sns.FacetGrid(train_data, hue="income>50K", size=4) \
   .map(plt.scatter, "hours.per.week", "age") \
   .add_legend();
plt.show();

# Xtrain_data = train_data.iloc[:,:-1]
# Ytrain_data = train_data.iloc[:,-1]
Ytrain_data = train_data['income>50K']
Xtrain_data = train_data.drop('income>50K',axis=1)

test_data['workclass'] = test_data['workclass'].replace('?','Private')
test_data['occupation'] = test_data['occupation'].replace('?','Other-service')
# test_data['occupation'] =  test_data['occupation'].replace('?','Exec-managerial')
test_data['native.country'] = test_data['native.country'].replace('?','United-States')
# test_data['new_feature_edu'] = test_data['education.num'].apply(new_feature)
ID = test_data['ID']
X_test = test_data.drop('ID',axis=1)

X_train,X_cv,Y_train,Y_cv = train_test_split(Xtrain_data,Ytrain_data,test_size=0.15,random_state=42,stratify=Ytrain_data)

wc_obj = CountVectorizer()
train_wc_pre = wc_obj.fit_transform(X_train['workclass'].values.astype('U'))
cv_wc_pre = wc_obj.transform(X_cv['workclass'].values.astype('U'))
test_wc_pre = wc_obj.transform(X_test['workclass'].values.astype('U'))
#or we can do .values.astype('str')
edu_obj = CountVectorizer()
train_edu_pre = edu_obj.fit_transform(X_train['education'].values.astype('U'))
cv_edu_pre = edu_obj.transform(X_cv['education'].values.astype('U'))
test_edu_pre = edu_obj.transform(X_test['education'].values.astype('U'))

ms_obj = CountVectorizer()
train_ms_pre = ms_obj.fit_transform(X_train['marital.status'].values.astype('U'))
cv_ms_pre = ms_obj.transform(X_cv['marital.status'].values.astype('U'))
test_ms_pre = ms_obj.transform(X_test['marital.status'].values.astype('U'))

occ_obj = CountVectorizer()
train_occ_pre = occ_obj.fit_transform(X_train['occupation'].values.astype('U'))
cv_occ_pre = occ_obj.transform(X_cv['occupation'].values.astype('U'))
test_occ_pre = occ_obj.transform(X_test['occupation'].values.astype('U'))

rel_obj = CountVectorizer()
train_rel_pre = rel_obj.fit_transform(X_train['relationship'].values.astype('U'))
cv_rel_pre = rel_obj.transform(X_cv['relationship'].values.astype('U'))
test_rel_pre = rel_obj.transform(X_test['relationship'].values.astype('U'))

race_obj = CountVectorizer()
train_race_pre = race_obj.fit_transform(X_train['race'].values.astype('U'))
cv_race_pre = race_obj.transform(X_cv['race'].values.astype('U'))
test_race_pre = race_obj.transform(X_test['race'].values.astype('U'))

sex_obj = OneHotEncoder()
train_sex_pre = sex_obj.fit_transform(X_train['sex'].values.reshape(-1, 1))
cv_sex_pre = sex_obj.transform(X_cv['sex'].values.reshape(-1, 1))
test_sex_pre = sex_obj.transform(X_test['sex'].values.reshape(-1, 1))

nc_obj = CountVectorizer()
train_nc_pre = nc_obj.fit_transform(X_train['native.country'].values.astype('U'))
cv_nc_pre = nc_obj.transform(X_cv['native.country'].values.astype('U'))
test_nc_pre = nc_obj.transform(X_test['native.country'].values.astype('U'))

std = StandardScaler()
train_age_pre_std = std.fit_transform(X_train['age'].values.reshape(-1,1))
cv_age_pre_std = std.transform(X_cv['age'].values.reshape(-1,1))
test_age_pre_std = std.transform(X_test['age'].values.reshape(-1,1))

std = StandardScaler()
train_fnlwgt_pre_std = std.fit_transform(X_train['fnlwgt'].values.reshape(-1,1))
cv_fnlwgt_pre_std = std.transform(X_cv['fnlwgt'].values.reshape(-1,1))
test_fnlwgt_pre_std = std.transform(X_test['fnlwgt'].values.reshape(-1,1))

std = StandardScaler()
train_en_pre_std = std.fit_transform(X_train['education.num'].values.reshape(-1,1))
cv_en_pre_std = std.transform(X_cv['education.num'].values.reshape(-1,1))
test_en_pre_std = std.transform(X_test['education.num'].values.reshape(-1,1))

std = StandardScaler()
train_cg_pre_std = std.fit_transform(X_train['capital.gain'].values.reshape(-1,1))
cv_cg_pre_std = std.transform(X_cv['capital.gain'].values.reshape(-1,1))
test_cg_pre_std = std.transform(X_test['capital.gain'].values.reshape(-1,1))

std = StandardScaler()
train_cl_pre_std = std.fit_transform(X_train['capital.loss'].values.reshape(-1,1))
cv_cl_pre_std = std.transform(X_cv['capital.loss'].values.reshape(-1,1))
test_cl_pre_std = std.transform(X_test['capital.loss'].values.reshape(-1,1))

std = StandardScaler()
train_hpw_pre_std = std.fit_transform(X_train['hours.per.week'].values.reshape(-1,1))
cv_hpw_pre_std = std.transform(X_cv['hours.per.week'].values.reshape(-1,1))
test_hpw_pre_std = std.transform(X_test['hours.per.week'].values.reshape(-1,1))

# std = StandardScaler()
# train_nfe_pre_std = std.fit_transform(X_train['new_feature_edu'].values.reshape(-1,1))
# cv_nfe_pre_std = std.transform(X_cv['new_feature_edu'].values.reshape(-1,1))
# test_nfe_pre_std = std.transform(X_test['new_feature_edu'].values.reshape(-1,1))

X_train_preprocessed_notnull_features = hstack((train_wc_pre,train_edu_pre,train_ms_pre,train_occ_pre,train_rel_pre,train_race_pre,train_sex_pre,train_nc_pre,train_age_pre_std,train_fnlwgt_pre_std,train_en_pre_std,train_cg_pre_std,train_cl_pre_std,train_hpw_pre_std))
X_cv_preprocessed_notnull_features = hstack((cv_wc_pre,cv_edu_pre,cv_ms_pre,cv_occ_pre,cv_rel_pre,cv_race_pre,cv_sex_pre,cv_nc_pre,cv_age_pre_std,cv_fnlwgt_pre_std,cv_en_pre_std,cv_cg_pre_std,cv_cl_pre_std,cv_hpw_pre_std))
X_test_preprocessed_notnull_features = hstack((test_wc_pre,test_edu_pre,test_ms_pre,test_occ_pre,test_rel_pre,test_race_pre,test_sex_pre,test_nc_pre,test_age_pre_std,test_fnlwgt_pre_std,test_en_pre_std,test_cg_pre_std,test_cl_pre_std,test_hpw_pre_std))

lr = LinearRegression()
lr.fit(X_train_preprocessed_notnull_features,Y_train)
train_pred = lr.predict(X_train_preprocessed_notnull_features)
cv_pred = lr.predict(X_cv_preprocessed_notnull_features)
mse = 0
print("Linear Regression")
for i in range(len(Y_train)):
    mse += (list(Y_train)[i]-train_pred[i])**2
print("train mse",mse/len(Y_cv))
mse = 0
for i in range(len(Y_cv)):
    mse += (list(Y_cv)[i]-cv_pred[i])**2
print("cv mse",mse/len(Y_cv))

X_train_preprocessed_notnull_features

pred_test = lr.predict(X_test_preprocessed_notnull_features)

result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = pred_test

result.to_csv("prediction_lr.csv",index=False)  #0.89351

c = [10**x for x in range(-2,1)] #c=1
print("Support Vector Classification")
for i in c:
    print("C = ",i)
    svc = SVC(C=i,probability=True,class_weight={0:1,1:3})
    svc.fit(X_train_preprocessed_notnull_features,Y_train)
    tr_pred = svc.predict(X_train_preprocessed_notnull_features)
    cv_pred = svc.predict(X_cv_preprocessed_notnull_features)
    mse = 0
    for i in range(len(Y_train)):
        mse += (list(Y_train)[i]-tr_pred[i])**2
    print("train mse",mse/len(tr_pred))
    mse = 0
    for i in range(len(Y_cv)):
        mse += (list(Y_cv)[i]-cv_pred[i])**2
    print("cv mse",mse/len(Y_cv))

pred_test = svc.predict_proba(X_test_preprocessed_notnull_features)

# pred_test
final_pred = []
for i in pred_test:
    if i[0]>i[1]:
        final_pred.append(1-i[0])
    else:
        final_pred.append(i[1])

result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = final_pred

result.to_csv("prediction_svc.csv",index=False) #0.89717

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from xgboost import XGBClassifier
# generate dataset

print("XGB Classifier")
max_dept = [3]
n_estimators = [200]
# et = [0.2,0.5,0.7,0.9]
# for e in [0.29,0.3,0.31]:
#     for depth in max_dept:
model = XGBClassifier(scale_pos_weight=weight_1,max_depth = 3, n_estimators=200,eta=0.3)
model.fit(X_train_preprocessed_notnull_features, Y_train)

yhat = model.predict_proba(X_train_preprocessed_notnull_features)
final_pred = []
for i in yhat:
    if i[0]>i[1]:
        final_pred.append(1-i[0])
    else:
        final_pred.append(i[1])
mse = 0
for i in range(len(Y_train)):
    mse += (list(Y_train)[i]-final_pred[i])**2
print("train mse",mse/len(Y_train))
# evaluate model
yhat = model.predict_proba(X_cv_preprocessed_notnull_features)
# pred_test
final_pred = []
for i in yhat:
    if i[0]>i[1]:
        final_pred.append(1-i[0])
    else:
        final_pred.append(i[1])
mse = 0
for i in range(len(Y_cv)):
    mse += (list(Y_cv)[i]-final_pred[i])**2
print("cv mse",mse/len(Y_cv))
# summarize performance
# print('Mean ROC AUC: %.5f' % mean(scores))

model = XGBClassifier(scale_pos_weight=1-weight_1,max_depth = 3, n_estimators=200)
model.fit(X_train_preprocessed_notnull_features, Y_train)

yhat = model.predict_proba(X_test_preprocessed_notnull_features)
# yhat = model.predict(X_test_preprocessed_notnull_features)

# pred_test
final_pred = []
for i in yhat:
    if i[0]>i[1]:
        final_pred.append(1-i[0])
    else:
        final_pred.append(i[1])

result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = final_pred

result.to_csv("prediction_xgbclassifer_1-weight1_3_200_0_1_without_outliers.csv",index=False)#0.92482 weight_1
#0.92756 -> 3 depth 200 estimators

params = {}
params['objective'] = 'reg:squarederror'
params['eval_metric'] = 'rmse'
params['eta'] = 0.05
params['max_depth'] = 5

d_train = xgb.DMatrix(X_train_preprocessed_notnull_features, label=Y_train)
watchlist = [(d_train, 'train')]
bst = xgb.train(params, d_train, 900,watchlist,verbose_eval=50)
pred_y_tr = bst.predict(d_train)
# print("r2_score train ",r2_score(Y1.values,pred_y_tr))

d_test = xgb.DMatrix(X_cv_preprocessed_notnull_features)
pred_y_test = bst.predict(d_test)
print('*'*100)

d_test = xgb.DMatrix(X_test_preprocessed_notnull_features)
pred_y_test = bst.predict(d_test)
print('*'*100)

result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = pred_y_test

result.to_csv("prediction_xgb.csv",index=False) # for eta 0.05 #0.92215

print("Random Forest Classifier")
tr = []
te = []
depth = [3,5,7]
n_estimators = [50,70,100,200,250]
for n_e in n_estimators:
  # for dep in depth:
      # print(n_e,dep)
      clf = RandomForestClassifier(max_depth=5, n_estimators=n_e, random_state=0)
      clf.fit(X_train_preprocessed_notnull_features, Y_train)
      train_pred = clf.predict_proba(X_train_preprocessed_notnull_features)
      final_pred_tr = []
      for i in train_pred:
          if i[0]>i[1]:
              final_pred_tr.append(1-i[0])
          else:
              final_pred_tr.append(i[1])
      mse = 0
      for i in range(len(Y_train)):
          mse += (list(Y_train)[i]-final_pred_tr[i])**2
      tr.append(mse/len(Y_train))
      print("train mse",mse/len(Y_train))
      cv_pred = clf.predict_proba(X_cv_preprocessed_notnull_features)
      final_pred_cv = []
      for i in cv_pred:
          if i[0]>i[1]:
              final_pred_cv.append(1-i[0])
          else:
              final_pred_cv.append(i[1])
      mse = 0
      for i in range(len(Y_cv)):
          mse += (list(Y_cv)[i]-final_pred_cv[i])**2
      te.append(mse/len(Y_cv))
      print("cv mse",mse/len(Y_cv))

plt.plot(n_estimators, tr, label="train loss")
plt.plot(n_estimators,te, label="cv loss")
plt.xlabel("n estimators with depth 5")
plt.ylabel("loss")
plt.title("Random Forest Classifier")
plt.legend()
plt.show()

clf = RandomForestClassifier(max_depth=5, n_estimators=100, random_state=0)
clf.fit(X_train_preprocessed_notnull_features, Y_train)
test_pred = clf.predict_proba(X_test_preprocessed_notnull_features)
final_pred_test = []
for i in test_pred:
    if i[0]>i[1]:
        final_pred_test.append(1-i[0])
    else:
        final_pred_test.append(i[1])

result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = final_pred_test

result.to_csv("prediction_rf.csv",index=False)#0.89920

print("Logistic Regression")
c = [10**x for x in range(-5,5)]
tr = []
te = []
for i in c:
    print("c = ",i)
    lr = LogisticRegression(C=i)
    lr.fit(X_train_preprocessed_notnull_features,Y_train)
    cv_pred = lr.predict(X_cv_preprocessed_notnull_features)
    train_pred = lr.predict(X_train_preprocessed_notnull_features)
    mse = 0
    for i in range(len(Y_train)):
        mse += (list(Y_train)[i]-train_pred[i])**2
    tr.append(mse/len(Y_train))
    print("train mse",mse/len(Y_train))
    mse = 0
    for i in range(len(Y_cv)):
        mse += (list(Y_cv)[i]-cv_pred[i])**2
    te.append(mse/len(Y_cv))
    print("cv mse",mse/len(Y_cv))

plt.plot(c, tr, label="train loss")
plt.plot(c,te, label="cv loss")
plt.xlabel("C value")
plt.ylabel("loss")
plt.legend()
plt.show()

Slr = LogisticRegression(C=1)
lr.fit(X_train_preprocessed_notnull_features,Y_train)
test_pred = lr.predict_proba(X_test_preprocessed_notnull_features)

# pred_test
final_pred = []
for i in test_pred:
    if i[0]>i[1]:
        final_pred.append(1-i[0])
    else:
        final_pred.append(i[1])

result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = final_pred

result.to_csv("prediction_Logistic_1.csv",index=False)#0.90661

weight_1

print("ANN")
X_train_nn = X_train_preprocessed_notnull_features.toarray()
X_cv_nn = X_cv_preprocessed_notnull_features.toarray()
X_test_nn = X_test_preprocessed_notnull_features.toarray()

model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(149,))),
model.add(tf.keras.layers.Dense(16,activation="relu"))
model.add(tf.keras.layers.Dense(32,activation="relu"))
model.add(tf.keras.layers.Dense(1,activation="sigmoid"))

model.compile(optimizer="adam",loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy','mse'])

model.fit(X_train_nn,Y_train,epochs=15,validation_data=(X_cv_nn,Y_cv))

final_pred = model.predict(X_test_nn)
result = pd.DataFrame(columns=['ID','Prediction'])
result['ID'] = ID
result['Prediction'] = final_pred

result.to_csv("prediction_naive_nn.csv",index=False)#0.90

